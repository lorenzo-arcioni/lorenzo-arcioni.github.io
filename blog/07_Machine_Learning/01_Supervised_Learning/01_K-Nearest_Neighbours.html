<!DOCTYPE html>
<html lang="en">
<head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VF7D8E2FE3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VF7D8E2FE3');
</script>

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" type="image/x-icon" href="../../../fav-icon.png">
    <link rel="stylesheet" href="../../../styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="../../../scripts.js" defer></script>

    <script>
        MathJax = {
          tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

    <style type="text/css">

        p {
            font-size: 20px;
        }

        h3 {
            font-size: 23px;
        }

        h2 {
            font-size: 26px;
        }
        h4 {
            font-size: 22px;
        }
        code {
            font-size: 20px;
            background-color: #1f1f1e;
        }
        ol,ul {
            font-size: 20px;
            margin-left: 40px;
        }
        pre {
            background-color: #1f1f1e;
        }
    </style>

    <title>Lorenzo Arcioni</title>
</head>

<body>
    <header>
        <nav>
            <div class="container">
                <div class="profile">
                    <img src="../../../lorenzoarcioni.png" alt="Immagine del profilo" class="profile-img">
                    <div class="name">
                        <h1 style="font-size: 32px;">Lorenzo Arcioni</h1>
                        <p class="subtitle" style="font-size: 20px;">Machine Learning Engineer | Data Scientist</p>
                        <p class="mobile_subtitle" style="font-size: 20px;">Machine Learning Engineer<br><br>Data Scientist</p>
                        <div class="social">
                            <a href="https://www.linkedin.com/in/lorenzo-arcioni-216b921b5/" target="_blank"><i class="fa fa-linkedin"></i></a>
                            <a href="https://github.com/lorenzo-arcioni" target="_blank"><i class="fa fa-github"></i></a>
                            <a href="https://www.kaggle.com/lorenzoarcioni" target="_blank"><strong>K</strong></a>
                            <a href="https://www.instagram.com/lorenzo_arcioni/" target="_blank"><i class="fa fa-instagram"></i></a>
                            <a href="https://medium.com/@lorenzo.arcioni2000/" target="_blank"><i class="fa fa-medium"></i></a>
                        </div>
                    </div>
                    <div class="blog-link"><p><a href="../../../index.html"><u>Home</u></a></p></div>
                </div>
                <hr>
            </div>
        </nav>
    </header>

    <main>
        <div class="container">
            <h2>k-Nearest Neighbors (kNN)</h2>
            <p>L'algoritmo dei k-nearest neighbors (kNN) è uno degli algoritmi più semplici e intuitivi da implementare per la classificazione e la regressione. Questo algoritmo si basa sul principio che i dati simili tendono a essere vicini nello spazio delle caratteristiche. Utilizzando la vicinanza tra i punti dati, kNN determina la classe o il valore di un nuovo campione analizzando i k punti dati più vicini (detti vicini) nel dataset di addestramento.</p>

            <p>La semplicità dell'algoritmo kNN risiede nella sua mancanza di una fase esplicita di addestramento: tutte le operazioni computazionali vengono eseguite solo quando si fa una previsione. Per questo motivo, kNN è spesso definito un algoritmo di apprendimento pigro (lazy learning), in contrapposizione agli algoritmi di apprendimento attivo (eager learning) che costruiscono modelli predittivi durante la fase di addestramento.</p>

            <br>
            <h3>Descrizione dell'algoritmo</h3>
            <p>Il processo generale dell'algoritmo kNN può essere descritto con i seguenti passaggi:</p>
            <br>
            <ol style="margin-left:40px;">
                <li>Scegliere il numero di vicini k.</li>
                <li>Calcolare la distanza tra il nuovo punto e tutti i punti nel dataset di addestramento.</li>
                <li>Ordinare le distanze in ordine crescente e selezionare i k punti più vicini.</li>
                <li>Per la classificazione, assegnare al nuovo punto la classe più frequente tra i k vicini. Per la regressione, assegnare al nuovo punto la media dei valori dei k vicini.</li>
            </ol>
            <br>
            <h3>Definizione formale</h3>
            <p>Per formalizzare matematicamente l'algoritmo k-Nearest Neighbors (kNN), consideriamo un dataset di addestramento \( \mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^N \), dove \( \mathbf{x}_i \in \mathbb{R}^d \) rappresenta un vettore di lunghezza \( d \) (un punto dati con \( d \) caratteristiche) e \( y_i \in \mathbb{R} \) (o \( y_i \in \{1, \ldots, C\} \) dove \(C \in \mathbb{N}\), per la classificazione) rappresenta un'istanza della variabile target associata.</p>

            <p>Assumiamo di aver osservato un insieme di \( n \) punti dati diversi. Queste osservazioni sono chiamate dati di addestramento perché li utilizziamo per addestrare il nostro modello su come stimare una funzione che ci permette di formalizzare la realtà di interesse. Lasciamo che \( x_{i,j} \) rappresenti il valore del \( j \)-esimo predittore, o input, per l'osservazione \( i \), dove \( i = 1, 2, \ldots, n \) e \( j = 1, 2, \ldots, d \). Di conseguenza, sia \( y_i \) la variabile di risposta per l'osservazione \( i \). I nostri dati di addestramento consistono in \( \{(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \ldots, (\mathbf{x}_n, y_n)\} \), dove \( \mathbf{x}_i = (x_{i,1}, x_{i,2}, \ldots, x_{i,d})^T \).</p>
            <br>
            <h3>Considerazioni sulla Scelta di k</h3>
            <p>La scelta del valore di k è cruciale per le prestazioni dell'algoritmo:</p>
            <ul>
                <li>Con k troppo piccolo, l'algoritmo può essere sensibile al rumore nei dati.</li>
                <li>Con k troppo grande, l'algoritmo può includere troppi punti che non sono realmente vicini.</li>
            </ul>
            <br>
            <h3>Metriche di Distanza</h3>
            <p>Le metriche di distanza più comuni utilizzate nel kNN sono:</p>
            <ul>
                <li>Distanza Euclidea: \[ d(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2} \]</li>
                <li>Distanza di Manhattan: \[ d(\mathbf{x}, \mathbf{y}) = \sum_{i=1}^{n} |x_i - y_i| \]</li>
                <li>Distanza di Minkowski: \[ d(\mathbf{x}, \mathbf{y}) = \left( \sum_{i=1}^{n} |x_i - y_i|^p \right)^{1/p} \]</li>
            </ul>
            <br>
            <h3>Vantaggi e Svantaggi</h3>
            <p>Vantaggi dell'algoritmo kNN:</p>
            <ul>
                <li>Semplice da implementare.</li>
                <li>Non richiede una fase di addestramento esplicita.</li>
                <li>Adatto a problemi sia di classificazione che di regressione.</li>
            </ul>
            <br>
            <p>Svantaggi dell'algoritmo kNN:</p>
            <ul>
                <li>Prestazioni dipendenti dalla scelta della metrica di distanza.</li>
                <li>Richiede spazio di memoria significativo per memorizzare tutti i dati di addestramento.</li>
                <li>Il tempo di predizione può essere elevato per dataset molto grandi.</li>
            </ul>
            <br>
            <h3>Applicazioni</h3>
            <p>L'algoritmo kNN viene utilizzato in diverse applicazioni, tra cui:</p>
            <ul>
                <li>Riconoscimento di immagini.</li>
                <li>Sistemi di raccomandazione.</li>
                <li>Riconoscimento di modelli.</li>
                <li>Rilevamento di anomalie.</li>
            </ul>
        </div>
    </main>
    

    <footer>
        <div class="container">
            <p>&copy; 2024 Lorenzo Arcioni. All rights reserved.</p>
        </div>
    </footer>
</body>
</html>